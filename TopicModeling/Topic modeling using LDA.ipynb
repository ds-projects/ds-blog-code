{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Introduction:\n",
    "\n",
    "Large amount of data is produced everyday and lot of this data is infact unstructured. Hence, it becomes important to be able to find mechanisms that will help us automatically organize these unstructured data. \n",
    "\n",
    "Topic modeling is one such mechanism that: \n",
    "\n",
    "- helps uncover hidden topics of various documents\n",
    "- allocate topics to these documents\n",
    "- provides a simpler way to analyze large volumes of unlabeled text \n",
    "\n",
    "As per __[KDNuggets post](http://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html)__ , 'Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection.' (Pls note that this post is inspired in part from the KD nuggets post attached)\n",
    "\n",
    "So, now we know what topic modeling is. What model can we use to find topics across documents of data? \n",
    "\n",
    "We will be using a model called Latent Dirichlet Allocation (LDA). Following are some of the highlights of LDA: \n",
    "\n",
    "- LDA is an unsupervised machine learning algorithm\n",
    "- It extracts key topics from collection of text / documents\n",
    "- These topics are represented in order of their importance / relevance to the document\n",
    "- LDA describes each document based on the ordered allocation of these topics\n",
    "\n",
    "Other important references used throughout these notebook are:\n",
    "- __[LDA explained in a video](https://www.youtube.com/watch?v=3mHy4OSyRf0)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How will we learn?\n",
    "\n",
    "In this example, we will try to extract topics from company reviews data from Indeed's Company pages. \n",
    "\n",
    "The example we will follow is split into two parts:\n",
    "\n",
    "- _Data Extraction_: We will extract company reviews from Indeed's website. Specifically, we will start by searching for companies that have posted 'Data Scientist' jobs in Austin, TX\n",
    "- _Explore Ratings_: Once we have all the information, we will make some nice charts around ratings of these companies\n",
    "- _Topic modeling_: Once we have data, we will go over the methodology to apply LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have separated the details of scraping reviews from Indeed in indeed.py file. \n",
    "\n",
    "Overall, the steps I have followed are:\n",
    "- Look up job details for a given job query and location\n",
    "- Extract name of the companies that have posted these jobs \n",
    "- Find the corresponding company pages on Indeed for these companies\n",
    "- Find ratings and reviews for these companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import indeed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the SERP url\n",
    "js_url = indeed.jobsearch_url('Data Scientist', 'Austin,TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract job details\n",
    "jobs_df = indeed.get_jobs(js_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the companies that posted these jobs, get company ratings\n",
    "ratings_df = indeed.get_comp_ratings(jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find reviews for companies that posted these jobs\n",
    "review_df = indeed.get_reviews(jobs_df)\n",
    "review_df = review_df.merge(jobs_df, on = 'comp_name', how = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had a chance to watch the attached youtube vide, there are three important items to be considered while building a LDA model. These are:\n",
    "\n",
    "- Setting the right window of context : Define minimum number of words to be present in each document. Recommended around 100 - 300. Since we are looking at review data, we will use min_word = 10\n",
    "\n",
    "- Include right words in the model: We will remove any words that appear in less than 20 documents and more than 10% of all our documents, remove any stop words, stem and lemmatize each of the words in every document\n",
    "\n",
    "- Capture widest range of topics: We will do this by setting the total number of topics we are interested in\n",
    "\n",
    "Following are the steps to preprocess our data before modeling:\n",
    "- *Tokenize* : split sentences into individual words\n",
    "- *Min Freq & Max freq*: remove any tokens or words that appear in less than 20 documents and in more than 10% of all the documents\n",
    "- *Stop words*: remove stop words from these tokens\n",
    "- *Convert each word to its base form*: This can be done by either 'stemming' or 'lemmatizing'. For our purposes, we will lemmatize the tokens and then stem the words. We will examine how lemmatization and stemming works and the difference between the two\n",
    "- *Create id-term dictionary*: We will later assign a random id to each word / token in our corpus\n",
    "- *Create BoW*: Gensim's LDA algorithm requires bag of words input. Hence, we will convert our id-term dictionary in the required input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words that appear in fewer than MIN_DOC_FREQ documents will be ignored\n",
    "MIN_DOC_FREQ = 20\n",
    "\n",
    "# if there are less than MIN_WORDS in a document, it is dropped\n",
    "MIN_WORDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from gensim import corpora, models, similarities\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, we will convert all the words into lower case\n",
    "reviews_lda_df = review_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Dell provides employees with learning opportunities to succeed.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample before changing the case\n",
    "reviews_lda_df['review_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dell provides employees with learning opportunities to succeed.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample after changing the review to lower case\n",
    "reviews_lda_df['review_text'][0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**:\n",
    "\n",
    "Tokenization is a way to segment our document into atomic modules of each word. \n",
    "\n",
    "There are several ways to tokenize your document. Ex. you can split() your document on spaces / '.'. In our case, we will use tokenize.regexp from nltk. \n",
    "\n",
    "You can see how this works in a fun interactive way here: try 'w+' at __[http://regexr.com/](http://regexr.com/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 8 words in our first review. The first few words are [u'Dell', u'provides', u'employees', u'with', u'learning', u'opportunities', u'to', u'succeed']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Running it only through first review for demo\n",
    "first_review = reviews_lda_df['review_text'][0]\n",
    "first_review_tokens = tokenizer.tokenize(first_review)\n",
    "\n",
    "print 'There are a total of', len(first_review_tokens), 'words in our first review. The first few words are', first_review_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopword removal:**\n",
    "\n",
    "Words like 'the', 'for', 'a', etc. do not add any meaningful value to our text. However, these words exist many times more than important and valuable words such as 'management'. Since these words (also called stopwords) are very common, including them will mean that our LDA algorithm will most likely provide us topics such as 'the' / 'a'. \n",
    "\n",
    "So, it is important to exclude these words from our reviews during our data preprocessing. \n",
    "\n",
    "Note that your choice of stopwords also depend on the context. Ex. 'an' is a typical stopword. However, let's say that 'pay and benefits' is a very commonly occuring term in our dataset. In that case, having a single topic as 'pay and benefit' vs two different topics like 'pay' and 'benefit' is more meaningful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "[u'all', u\"she'll\", u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its']\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "nltk_stop_wds = stopwords.words('english')\n",
    "get_stop_wds = get_stop_words('en')\n",
    "all_stop_words = list(set(nltk_stop_wds + get_stop_wds))\n",
    "all_stop_words += ['.', '...', ',', '(', ')', ':', '`', '``', ';']\n",
    "all_stop_words += [\"'s\", \"n't\"]\n",
    "\n",
    "print(len(set(all_stop_words)))\n",
    "print(all_stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Dell', u'provides', u'employees', u'learning', u'opportunities', u'succeed']\n"
     ]
    }
   ],
   "source": [
    "fst_rev_wo_stp = [token for token in first_review_tokens if not token in all_stop_words]\n",
    "print(fst_rev_wo_stp[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**: \n",
    "\n",
    "Stemming is a mechanism in NLP that helps convert any given word to it's base form. For example, 'breaking' and 'breaker' will be reduced to their base form, 'run'. \n",
    "\n",
    "We will be using Snowball stemmer in our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem the first review\n",
    "# Instantiate a Snowball stemmer\n",
    "sb_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'dell', u'provid', u'employe', u'learn', u'opportun', u'succeed']\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = [sb_stemmer.stem(token) for token in fst_rev_wo_stp]\n",
    "print stemmed_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization:**\n",
    "\n",
    "__[These stackoverflow answers](https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming)__  provide a good explanation of difference between stemming and lemmatization as follows: \n",
    "\n",
    "'Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma'\n",
    "\n",
    "Ex: \n",
    "\n",
    "- The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up\n",
    "\n",
    "- The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "- The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatize the first review\n",
    "# Instantiate a Wordnet lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Dell', u'provides', u'employee', u'learning', u'opportunity', u'succeed']\n"
     ]
    }
   ],
   "source": [
    "lamma_tokens = [wordnet_lemmatizer.lemmatize(token) for token in fst_rev_wo_stp]\n",
    "print lamma_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting this together\n",
    "\n",
    "Before we apply our LDA model, we need to create list of documents (list of lists) wherein each document is tokenized, has stop words removed from, is stemmed and lemmatized. \n",
    "\n",
    "Then, we will create a term-document frequency matrix from this list and pass it to our LDA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(review_df):\n",
    "    #First, we will create list of documents (list of each review in our dataset)\n",
    "    doc_list = [review for review in review_df['review_text'].dropna()]\n",
    "    \n",
    "    #lowercase all the documents\n",
    "    doc_list = [doc.lower() for doc in doc_list]\n",
    "    \n",
    "    #tokenize every document\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc_list = [tokenizer.tokenize(content) for content in doc_list]\n",
    "    \n",
    "    #remove any stop words\n",
    "    nltk_stop_wds = stopwords.words('english')\n",
    "    get_stop_wds = get_stop_words('en')\n",
    "    all_stop_words = list(set(nltk_stop_wds + get_stop_wds))\n",
    "    all_stop_words += ['.', '...', ',', '(', ')', ':', '`', '``', ';']\n",
    "    all_stop_words += [\"'s\", \"n't\"]\n",
    "    doc_list = [[token for token in review_doc if token not in all_stop_words] for review_doc in doc_list]\n",
    "\n",
    "    #lemmatize words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    doc_list = [[wordnet_lemmatizer.lemmatize(token) for token in doc] for doc in doc_list]\n",
    "    \n",
    "    #stem words\n",
    "#     sb_stemmer = SnowballStemmer('english')\n",
    "#     doc_list = [[sb_stemmer.stem(token) for token in doc] for doc in doc_list]\n",
    "    \n",
    "    #keep those documents where the number of tokens is 20 (MIN_WORDS) or more \n",
    "    doc_list = [doc for doc in doc_list if len(doc) >= MIN_WORDS]\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete in 0.991232156754 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "preprocessed_reviews = preprocess(reviews_lda_df)\n",
    "end = time.time()\n",
    "print 'Preprocessing complete in', str(end - start), 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Id Term dictionary\n",
    "\n",
    "As mentioned at the beginning, we will now convert our preprocessed reviews into a id-term dictionary wherein every word in our preprocessed reviews corpus will have a randomly assigned id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3853 unique tokens: [u'gatekeeper', u'similarity', u'consolidated', u'personally', u'yellow']...)\n"
     ]
    }
   ],
   "source": [
    "# Now, we will create the id-term dictionary as mentioned in the steps at\n",
    "# the beginning\n",
    "reviews_dict = corpora.Dictionary(preprocessed_reviews)\n",
    "print reviews_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning of the tutorial, we need to set 'window of context'. \n",
    "\n",
    "One of the elements of setting window of context is to exclude tokens that appear in less than 20 documents and appear in more than 10% of the documents. \n",
    "\n",
    "We are currently working on a small corpus right now and hence, we may not want to reduce our corpus even further. However, let's see what impact this step will have on our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(217 unique tokens: [u'atmosphere', u'help', u'office', u'food', u'indeed']...)\n",
      "top terms:\n",
      "[(u'atmosphere', 0), (u'help', 1), (u'office', 2), (u'indeed', 3), (u'lack', 4), (u'find', 5), (u'young', 6), (u'competitive', 7), (u'month', 8), (u'enjoyable', 9)]\n"
     ]
    }
   ],
   "source": [
    "reviews_dict.filter_extremes(no_below=30, no_above=0.15) # changes reviews_dict in place\n",
    "print(reviews_dict)\n",
    "print(\"top terms:\")\n",
    "print(sorted(reviews_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we went from **3040 tokens** to **237 tokens**. That is a big difference. Let's continue with this small corpus and see what we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "\n",
    "As mentioned at the beginning, LDA algorithm requires bag of words representation as it's input. \n",
    "\n",
    "If we represent each word in each document of our corpus as (id, freq_count of that word in the corpus), we get bag of word representation. __[Here's a short (2 min) clip](https://www.youtube.com/watch?v=OGK9SHt8SWg)__ explaining bag of words with example. \n",
    "\n",
    "We will use doc2bow() method to convert list of documents into bow output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_corpus = [reviews_dict.doc2bow(review_doc) for review_doc in preprocessed_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LDA modeling, we need to choose the number of topics before we start modeling. \n",
    "\n",
    "Ideally, this depends on the context. For ex, if you are trying to build topic model on documents from news website and if you know that that news covers mainly four topics: politics, sports, world news and finance, you will choose your number of topics to be 4. \n",
    "\n",
    "For our case, since we do not have such context for now, let's start with looking out for around 4 topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 37.9830701351 seconds to complete LDA model\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lda_model = models.LdaModel(reviews_corpus,alpha='auto', \n",
    "                                   num_topics=4, id2word = reviews_dict, \n",
    "                                   passes=20)\n",
    "end_time = time.time()\n",
    "print 'It took', str(end_time - start_time), 'seconds to complete LDA model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show me the topics!\n",
    "\n",
    "Now that we have a LDA model, let's look at the top 10 words in the reviews associated with 4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.027*student + 0.026*indeed + 0.019*research + 0.018*learn + 0.017*opportunity + 0.017*experience + 0.017*make + 0.016*university + 0.016*austin + 0.015*pay'),\n",
       " (1,\n",
       "  u'0.046*sale + 0.034*indeed + 0.027*year + 0.022*product + 0.016*office + 0.016*training + 0.014*call + 0.014*experience + 0.013*many + 0.013*position'),\n",
       " (2,\n",
       "  u'0.029*office + 0.022*everyone + 0.021*hour + 0.016*ibm + 0.016*well + 0.016*really + 0.015*pay + 0.015*life + 0.014*training + 0.014*help'),\n",
       " (3,\n",
       "  u'0.039*learned + 0.031*hardest + 0.027*enjoyable + 0.021*typical + 0.020*always + 0.018*different + 0.016*hard + 0.015*best + 0.014*high + 0.014*customer')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics = 4,num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "We just saw how to build a simple LDA model on reviews from jobseekers. \n",
    "\n",
    "In order to get more interesting topics, we can try to focus on:\n",
    "\n",
    "- find topics in reviews by company \n",
    "- find topics by average rating of company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
